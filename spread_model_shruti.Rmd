---
title: "Playoffs 2 Spread Model"
author: "Shruti Gopalswamy"
output: html_notebook
---

# Importing the Data
```{r}
games <-  read.csv("https://raw.githubusercontent.com/mattymo18/STOR-538-Project2-2021/master/Source-Data/games.csv")
game_details <- read.csv("https://raw.githubusercontent.com/mattymo18/STOR-538-Project2-2021/master/Source-Data/games_details.csv")
teams <- read.csv("https://raw.githubusercontent.com/mattymo18/STOR-538-Project2-2021/master/Source-Data/teams.csv")

all_game_data <- read.csv("https://raw.githubusercontent.com/john-lickteig/playoff2/master/all_game_data.csv")
game_details_cleaned <- read.csv("https://raw.githubusercontent.com/john-lickteig/playoff2/master/game_details_cleaned.csv")
games_cleaned <- read.csv("https://raw.githubusercontent.com/john-lickteig/playoff2/master/games_cleaned.csv")

final_game_data_with_rolling_avg <- read.csv("https://raw.githubusercontent.com/john-lickteig/playoff2/master/final_game_data_with_rolling_avg.csv")
```



## Variable Selection Using Stepwise Selection Method

Starting with a basic linear regression model with all the numeric variables as predictors (except PTS_home and PTS_away as those are used to calculate Spread directly) as the baseline, and using the stepwise selection method with Cp as the criteria to narrow predictors down to the most significant one(s) for predicting Spread, we came up with the linear regression model with equation 

Spread = 0.09071(OREB_away) + 1.64248 (with AIC=2.19) 


Thus, using stepwise selection we found that `OREB_away` alone to be the most significant predictor of `Spread`. 

```{r}
# Fit the full model as the baseline linear reg. model
Full=lm(Spread~Total+OREB_home+OREB_away+OREB, data=all_game_data)

# Find the MSE for the baseline full model
MSE=(summary(Full)$sigma)^2

# Start the stepwise selection process with a model with no predictors
none=lm(Spread~1,data=all_game_data)
step(none,scope=list(upper=Full),scale=MSE)
```




## Variable Selection with New Rolling Avg Variable Using Stepwise Selection Method

Starting with a basic linear regression model with all the numeric team-level variables as predictors (except PTS_home and PTS_away as those are used to calculate Spread directly) as the baseline linear model, and using the stepwise selection method with Cp as the criteria to narrow predictors down to the most significant one(s) for predicting Spread, we came up with the linear regression model with equation below.

Spread = 3.151e-02(PM_home) + 1.030e-04(AST_away) + -1.099e-04(FGA_home) + -2.899e-01(FG3M_away) + 2.898e-01(Total) + -5.796e-01(FGM) + -2.898e-01(FG3M_home) + -2.897e-01(FTM) + -1.685e-01(PM_away) + -9.262e-05(FTA_away) + 8.811e-05 (FG3A_away) + -9.853e-05(TO_home) + 9.583e-05(AST_home) + 8.358e-03 

AIC decreased from 4562503013 (empty model) to 9527.17 (above lm)

Thus, using stepwise selection we found that the team-level variables `PM_home`, `AST_away`, `FGA_home`, `FG3M_away`, `Total`, `FGM`, `FG3M_home`, `FTM`, `PM_away`, `FTA_away`, `FG3A_away`, `TO_home`, `AST_home` to be the most significant predictors of `Spread`. 

```{r}
# Fit the full model as the baseline linear reg. model
Full=lm(Spread ~ . - X - GAME_DATE_EST - Home.Team - Away.Team - PTS_home - PTS_away, data=final_game_data_with_rolling_avg)

# Find the MSE for the baseline full model
MSE=(summary(Full)$sigma)^2

# Start the stepwise selection process with a model with no predictors
none=lm(Spread~1,data=final_game_data_with_rolling_avg)
step(none,scope=list(upper=Full),scale=MSE)
```


```{r}
shruti_spread_model_1 = lm(formula = Spread ~ PM_home + AST_away + FGA_home + FG3M_away + Total + FGM + FG3M_home + FTM + PM_away + FTA_away + FG3A_away + TO_home + AST_home, data = final_game_data_with_rolling_avg)

summary(shruti_spread_model_1)
```

## Checking for multicollinearity between the predictors included in shruti_spread_model_1

Since the focus of the spread model is on making inferences regarding the relative importance of the predictors, and would likely be used to make predictions in a different basketball data set, in which the correlations may be different, a high VIF may be problematic. A high VIF makes it hard to disentangle the relative importance of predictors to the response, spread, in a model, and leads to a more inflated std. error (and thus variance/R^2), thereby a smaller chance that the correlation coefficient of a predictor is statistically significant.

There are no VIF greater than 5 (or > 2,5, if taking a more conservative approach) for any of the predictors included in the spread model, indicating a lack of inflation due to multicollinearity within the linear model.

```{r}
round(cor(final_game_data_with_rolling_avg[5:50]),2)
```
```{r}
vif<- function(model, ...) {  
  V <- summary(model)$cov.unscaled
  Vi <- crossprod(model.matrix(model))
	nam <- names(coef(model))
  if(k <- match("(Intercept)", nam, nomatch = F)) {
		v1 <- diag(V)[-k]
		v2 <- (diag(Vi)[-k] - Vi[k, -k]^2/Vi[k,k])
		nam <- nam[-k]
	} else {
		v1 <- diag(V)
		v2 <- diag(Vi)
		warning("No intercept term detected.  Results may
surprise.")
	}
	structure(v1*v2, names = nam)
}

vif(shruti_spread_model_1)
```

# converting response variable to be points away & points home (to account for spread and total without repitition)

Since PTS_home and PTS_away are represent the values that go into calculating Spread and Total, a model combines two individual models, one that predict PTS_home alone, with another that predicts PTS_away alone, (both excluding the Spread and Total variables, as those are the variables we are looking to predict indirectly) may be less repetitive and could provide better predictive power while minimizing the number of predictors.

# Stepwise on PTS_home model

Starting with a basic linear regression model with all the numeric team-level variables as predictors (except Spread and Total) as the baseline linear model. Then using the stepwise selection method with Cp as the criteria to narrow predictors down to the most significant one(s) for predicting PTS_away, we came up with the linear regression model with equation below.

Call:
lm(formula = (PTS_home) ~ FGM_home + FTM_home + FG3M_home + OREB + FG3A_home, data = final_game_data_with_rolling_avg)

Coefficients:
(Intercept)     FGM_home     FTM_home    FG3M_home         OREB    FG3A_home  
  0.0069149    1.9999435    1.0001814    0.9992511   -0.0004833    0.0004289


After performing the stepwise selection algorithm, the AIC decreased from 125036351 (for the empty model) to 3396.38 (above lm)

Thus, using the stepwise selection algorithm, we found that the team-level variables `FGM_home`, `FTM_home`, `FG3M_home`, `OREB`, `FG3A_home`, to be the most significant predictors of `PTS_home`


```{r}
# Fit the full model as the baseline linear reg. model
Full=lm((PTS_home) ~ . - X - GAME_DATE_EST - Home.Team - Away.Team - Spread - Total - PTS_away, data=final_game_data_with_rolling_avg)

# Find the MSE for the baseline full model
MSE=(summary(Full)$sigma)^2

# Start the stepwise selection process with a model with no predictors
none=lm((PTS_home)~1,data=final_game_data_with_rolling_avg)
step(none,scope=list(upper=Full),scale=MSE)
```


# Stepwise on PTS_away model

Starting with a basic linear regression model with all the numeric and binary team-level variables as predictors (except Spread and Total) as the baseline linear model. Then using the stepwise selection method with Cp as the criteria to narrow predictors down to the most significant one(s) for predicting PTS_away, we came up with the linear regression model with equation below.

Call:
lm(formula = (PTS_away) ~ FGM_away + FTM_away + FG3M_away + LAST_5_AVG_PTS_away + BLK_away + HOME_TEAM_WINS, data = final_game_data_with_rolling_avg)

Coefficients:
        (Intercept)             FGM_away             FTM_away            FG3M_away  
         -0.0117257            1.9999353            0.9997899            1.0002729  
LAST_5_AVG_PTS_away             BLK_away       HOME_TEAM_WINS  
          0.0001852           -0.0005272           -0.0024845  

After performing the stepwise selection algorithm, the AIC decreased from 513396505 (for the empty model) to 3171.61 (above lm)

Thus, using the stepwise selection algorithm, we found that the team-level variables `FGM_away`, `FTM_away`, `FG3M_away`, `LAST_5_AVG_PTS_away`, `BLK_away`, `HOME_TEAM_WINS` to be the most significant predictors of `PTS_away`


```{r}
# Fit the full model as the baseline linear reg. model
Full=lm((PTS_away) ~ . - X - GAME_DATE_EST - Home.Team - Away.Team - Spread - Total - PTS_home, data=final_game_data_with_rolling_avg)

# Find the MSE for the baseline full model
MSE=(summary(Full)$sigma)^2

# Start the stepwise selection process with a model with no predictors
none=lm((PTS_away)~1,data=final_game_data_with_rolling_avg)
step(none,scope=list(upper=Full),scale=MSE)
```


# R^2 for PTS_home model
Multiple R-squared:  0.9999,	Adjusted R-squared:  0.9999 

```{r}
shruti_PTS_home_model_1 = lm(formula = (PTS_home) ~ FGM_home + FTM_home + FG3M_home + OREB + FG3A_home, data = final_game_data_with_rolling_avg)

summary(shruti_PTS_home_model_1)
```

# R^2 for PTS_home model
Multiple R-squared:      1,	Adjusted R-squared:      1 

```{r}
shruti_PTS_away_model_1 = lm(formula = (PTS_away) ~ FGM_away + FTM_away + FG3M_away + LAST_5_AVG_PTS_away + BLK_away + HOME_TEAM_WINS, data = final_game_data_with_rolling_avg)

summary(shruti_PTS_away_model_1)
```

# Using VIF to assess presence of multi-colinnearity of individual PTS_away and PTS_home models

Both the linear models for PTS_home and PTS_away had very high R^2 values, which seems to indicate that they are statistically significant models. However, such a high R^2 value for both models may indicate the presence of multicollinearity, skewing the likelihood of statistical significance among the correlation coefficients of the predictors, and thus the overall model.

Since the focus of both the spread and total models is on making inferences regarding the relative importance of the predictors, and would likely be used to make predictions in a different basketball data set, in which the correlations may be different, a high VIF may be problematic. A high VIF makes it hard to disentangle the relative importance of predictors to the response, spread or total, in a model, and leads to a more inflated std. error (and thus variance/R^2), thereby a smaller chance that the correlation coefficient of a predictor is statistically significant.

There are no VIF greater than 5 (or > 2.5, if taking a more conservative approach) for any of the predictors included in the spread model, indicating a lack of inflation due to multicollinearity within the linear model. However, taking a more conservative approach, predictors `FG3M_home` (with a VIF of 2.833692) and `FG3A_home` (with VIF of 2.472331)in the model for `PTS_home` as response, have VIF values greater than 2.5, which indicates that the correlation between just those two predictors (the correlation coefficient between the two is 0.75) may be too high enough to the point that the relative importance of both of predictors is not meaningful in context of the response variable, and thus the more inflated std. error and variance indicates that the seemingly strong correlation coefficient of 0.75 between the two predictors is not likely to be statistically significant. in other words, the two predictors are too closely related in context, that they do not add much meaningful value in terms of statistical significance to the predictive model.

# could try removing the high VIF variables and see if overall R^2 of model (run summary() on the model) is higher. if not keep the high VIF variables.

```{r}
round(cor(final_game_data_with_rolling_avg[5:50]),2)
```


```{r}
vif<- function(model, ...) {  
  V <- summary(model)$cov.unscaled
  Vi <- crossprod(model.matrix(model))
	nam <- names(coef(model))
  if(k <- match("(Intercept)", nam, nomatch = F)) {
		v1 <- diag(V)[-k]
		v2 <- (diag(Vi)[-k] - Vi[k, -k]^2/Vi[k,k])
		nam <- nam[-k]
	} else {
		v1 <- diag(V)
		v2 <- diag(Vi)
		warning("No intercept term detected.  Results may
surprise.")
	}
	structure(v1*v2, names = nam)
}
```

```{r}
vif(shruti_PTS_home_model_1)
```

```{r}
vif(shruti_PTS_away_model_1)
```

# R^2 for PTS_home model with teh high VIF predictors removed

Multiple R-squared:  0.9208,	Adjusted R-squared:  0.9208 

After removing the predictors `FG3M_home` and `FG3A_home`, which could be considered to have a high VIF values, if gauging with more strict boundaries, the new `PTS_home` model actually had a lower R^2 value.

```{r}
shruti_PTS_home_model_2 = lm(formula = (PTS_home) ~ FGM_home + FTM_home + OREB, data = final_game_data_with_rolling_avg)

summary(shruti_PTS_home_model_2)
```
